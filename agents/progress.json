{
  "issues": [
    {
      "name": "VL Embeddings: plan for Qwen3-VL-Embedding (self-host first; hosted later)",
      "description": "Goal: support a single fused VL embedding per entity (gallery/video) computed from (text_context + N images/frames), stored as one row per (entity_type, entity_id, model) and indexed for KNN.\n\nImportant constraint: Qwen/Qwen3-VL-Embedding-* is not currently available as a hosted API endpoint we can depend on. Design embeddingkit so that:\n- it can run against a future hosted provider (URL-first inputs), and\n- it can self-host the HuggingFace model as an internal service when needed.\n\nReference script (pool-last-token + L2 normalize): https://huggingface.co/Qwen/Qwen3-VL-Embedding-8B/blob/main/scripts/qwen3_vl_embedding.py",
      "tasks": [
        "=== SPEC ===",
        "[ ] Define canonical VL query contract: text-only, image-only, and text+images all produce ONE query vector in the same model space",
        "[ ] Define entity VL contract: ONE stored vector per entity per model (no per-asset vectors in v1)",
        "[ ] Define hard limits for VL inputs: max pages/frames, max total pixels, and max token length",
        "",
        "=== MANGA/GALLERY STRATEGY ===",
        "[ ] Decide page selection policy for galleries (e.g. cover + uniform samples; cap at N)",
        "[ ] Decide whether to treat pages as \"video frames\" to reuse the HF script path (recommended for multi-page fusion)",
        "[ ] Define chunking fallback for large galleries (e.g. 20 pages => 4 chunks of 5; embed each chunk; average+normalize)",
        "",
        "=== HENTAI0/VIDEO STRATEGY ===",
        "[ ] Use existing extracted frames as the VL inputs (no MP4 upload); cap frames and sample deterministically",
        "[ ] Decide frame ordering and sampling (keyframes vs uniform; stable ordering for idempotency)",
        "",
        "=== EMBEDDINGKIT API DESIGN ===",
        "[ ] Extend `embeddingkit/vl` with a \"fused\" embed interface that accepts (text + N image URLs/bytes) and returns ONE vector",
        "[ ] Keep URL-first asset fetching (presigned URLs) with bytes fallback for providers that cannot fetch URLs",
        "[ ] Provide a simple fusion helper for chunk-level fusion (avg + L2 normalize) when inputs are chunked",
        "",
        "=== SELF-HOST (FIRST IMPLEMENTATION) ===",
        "[ ] Create a small optional helper package (or reference impl) to run Qwen3-VL-Embedding-8B locally (Transformers/PyTorch) behind HTTP",
        "[ ] Implement request format for text + frames/pages; apply the same pooling as the HF script (last token + normalize)",
        "[ ] Add guardrails: per-request max frames/pages, max pixels, and timeout/cancellation",
        "",
        "=== HOSTED PROVIDER (FUTURE) ===",
        "[ ] Add a provider adapter slot for a future hosted Qwen3-VL-Embedding endpoint (URL-first)",
        "[ ] Ensure model registry supports multiple VL models (2B/8B) at native dims without schema changes",
        "",
        "=== STORAGE/SEARCH (APP-OWNED) ===",
        "[ ] Document expected Postgres schema pattern for storing VL vectors (row-based, halfvec(K), HNSW indexes, and optional 2-stage binary oversample+rescore)",
        "[ ] Document how apps switch active VL model and how to backfill safely"
      ],
      "completed": false
    },
    {
      "name": "Embedding/Search: extract to shared embeddingkit library",
      "description": "Goal: extract embedding generation + vector search core into a reusable Go module that both `~/doujins` and `~/hentai0` can consume. Keep domain-specific text composition and API response shapes inside each app, but share: (1) OpenAI-compatible embedding client + model mapping + dims, (2) pgvector/halfvec SQL/query helpers, (3) optional task queue + River worker primitives.\n\nName: `embeddingkit` (Go module `github.com/doujins-org/embeddingkit`).\n\n## Scope (what embeddingkit owns)\n\n- Embedding clients\n  - OpenAI-compatible text embedding client (DeepInfra/DashScope base URLs, model mapping, dims, timeouts/retries).\n  - VL embedding interface (text+image / text+frames) behind an abstraction; provider-specific impls live in embeddingkit if stable.\n- Vector storage + query helpers\n  - Canonical pgvector/halfvec helpers (SQL fragments + Bun helpers).\n  - Two-stage retrieval helpers (binary_quantize oversample + halfvec cosine rescoring).\n  - Optional post-processing helpers (MMR/per-series cap).\n- Task/work primitives (optional but recommended)\n  - Generic task tables + repos for “embed entity X with model Y”.\n  - River worker(s) that process tasks, call back into the app for text/asset materialization, and upsert embeddings.\n\n## Non-scope (what stays in apps)\n\n- Domain text templates (how to turn a gallery/video into text) and any per-entity prompt/template logic.\n- Domain routing/API response formats.\n- Domain triggers (what changes require re-embedding); apps enqueue tasks, embeddingkit just provides helpers.\n\n## Tables/migrations contract\n\n- embeddingkit ships Postgres migrations (embedded via embed.FS) for its generic tables:\n  - `embedding_tasks`\n  - `entity_embeddings` / `entity_embedding_assets` (rows keyed by (entity_type, entity_id, model, ...))\n  - optional helper views/index helpers if needed\n- Applications run these migrations via migratekit as part of their normal migration flow.\n- Embeddings are language-agnostic in storage and search: the same stored vectors are used for multilingual queries.\n\n## Integration contract (how apps interact)\n\nApps provide:\n- DB handle + config (enabled models, dims, oversample factor, schema prefix).\n- Callbacks/interfaces:\n  - `TextDocumentBuilder(entityID) -> string` (language-agnostic)\n  - `AssetLister(entityID) -> []AssetRef` + `AssetFetcher(AssetRef) -> bytes/stream` (for VL)\n  - optional: metadata provider (deleted/stale/languages)\n- Apps register embeddingkit River workers, enqueue tasks, and call embeddingkit search helpers; apps hydrate final entities by ID.\n\nThis is a planning + refactor initiative intended to reduce duplicated logic and keep semantic search behavior consistent across projects.",
      "tasks": [
        "=== INVENTORY (DONE ENOUGH TO START) ===",
        "[x] Identify doujins embedding storage + workers + search integration points",
        "[x] Identify hentai0 embedding storage + client + search integration points",
        "",
        "=== DESIGN (NEXT) ===",
        "[x] Decide the target Postgres embedding storage standard (use halfvec(K) everywhere; dimension enforced by type + model config)",
        "[x] Decide normalization strategy (normalize at write + query; cosine similarity/distance everywhere)",
        "[ ] Decide VL embedding scope (doujins): which images to embed per gallery (cover only vs cover+N representative pages)",
        "[ ] Decide VL embedding scope (hentai0): which frames to embed per video (keyframes vs uniform sampling, how many)",
        "[ ] Decide VL embedding strategy: produce a SINGLE fused Qwen3-VL embedding per entity from (text_context + up to N images/frames) and index/search only that one vector (no per-asset vectors in the first iteration)",
        "[ ] Decide VL instruction templates per task (text→image search, image→image similarity, multilingual queries)",
        "[ ] Decide VL storage layout: store ONE embedding row per (entity_type, entity_id, model) (not columns) for fused VL vectors; support both Qwen3-VL-Embedding-2B (halfvec(2048)) and -8B (halfvec(4096)) at native dims",
        "[x] Verify provider support + endpoints for Qwen3-VL (Result: no hosted Qwen3-VL-Embedding endpoint found yet; build embeddingkit VL around URL-first abstractions + pluggable provider impls; DashScope can remain an optional fallback via its vision embedding models if desired)",
        "[x] Define `embeddingkit` package boundaries (client vs pg helpers vs worker/task primitives)",
        "[x] Define minimal interfaces/callbacks so entity-specific text building stays in apps",
        "",
        "=== IMPLEMENTATION (PHASED) ===",
        "[x] Create new Go module `github.com/doujins-org/embeddingkit` (separate repo or shared workspace module)",
        "[x] Move OpenAI-compatible embedding client + canonical model mapping into `embeddingkit/embedder`",
        "[x] Add VL embedder interface (no provider impl yet) that supports text+image inputs",
        "[x] Migrate doujins to use `embeddingkit` for embedding API calls (keep DB schema as-is initially)",
        "[x] Migrate hentai0 to use `embeddingkit` for embedding API calls (replace custom DeepInfra client)",
        "[ ] Add Postgres schema for VL embeddings (new table(s) keyed by (gallery_id, model, asset_kind, asset_key, frame_idx) + halfvec(2048/4096) columns)",
        "[ ] Add Postgres schema for aggregate VL embeddings (one row per gallery/video per model, used for fast search)",
        "[ ] Add VL embedding tasks + worker pipeline (download asset(s) from S3/MinIO, preprocess, embed, store, mark stale)",
        "[ ] Add bounded-rate asset fetch + preprocessing (image resizing/format normalization; video frame extraction)",
        "",
        "=== SEARCH (PHASED) ===",
        "[x] Extract reusable pgvector/halfvec query helpers (Bun + raw SQL fragments) into `embeddingkit/pg`",
        "[x] Migrate doujins semantic search repo to use `embeddingkit/pg` helpers",
        "[ ] Migrate hentai0 semantic search repo to use `embeddingkit/pg` helpers (blocked: hentai0 still stores embeddings as `vector(1536)`; `embeddingkit/pg` currently targets `halfvec(K)`)",
        "[x] Implement 2-stage retrieval for text embeddings: binary_quantize(halfvec) HNSW oversample (hamming) + halfvec cosine rescoring",
        "[ ] Implement 2-stage retrieval for VL embeddings: binary_quantize(halfvec) HNSW oversample (hamming) + halfvec cosine rescoring",
        "[x] Add HNSW indexes for binary quantized search (expression index on binary_quantize(embedding)::bit(K))",
        "[ ] Add ability to switch search modality (text vs VL vs blended) per endpoint/config",
        "[ ] Define blending strategy (max, weighted sum, RRF) and thresholds for fallback (lexical-only when trigram is high confidence)",
        "[x] Add \"more like this\" similarity search helper: given an entity_id, fetch its embedding and return nearest neighbors (excluding self), with modality/model selection",
        "[ ] Add diversity control to search pipeline (MMR and/or per-series cap as a post-process step)",
        "[ ] Add an evaluation harness (hand-written query→expected results set; compute basic metrics like recall@k/MRR)",
        "[x] Implement L2 normalization in doujins embedding generation + search query vectors",
        "",
        "=== WORKERS/TASKS (OPTIONAL BUT RECOMMENDED) ===",
        "[x] Define generic embedding task table pattern + repo helpers in `embeddingkit/tasks`",
        "[x] Define River worker implementations in `embeddingkit/river` that are entity-agnostic via callbacks",
        "[ ] Port hentai0 from \"loop missing versions\" to task-table + batch worker pattern (parity with doujins)",
        "",
        "=== DOCUMENTATION ===",
        "[x] Document integration patterns for both apps (migrations, startup hooks for active-model view/indexes, operational knobs)",
        "[x] Document rollout plan (dual-write/cutover model changes) and how to keep search stable during migrations"
      ],
      "completed": false
    }
  ]
}
